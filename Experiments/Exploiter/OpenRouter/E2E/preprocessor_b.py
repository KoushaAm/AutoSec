#!/usr/bin/env python3
"""
sarif_preprocessor.py

Parse SARIF results and extract structured information useful for an Exploiter agent:
- ruleId, message, main location (file, lines, cols)
- partial fingerprints
- code flow (ordered threadFlow locations) with step_index
- related locations
- light-weight data-flow extraction (variable names, taint labels in messages)
- lightweight control-flow hints (branch counts, presence of condition-like messages)
- classification heuristics: source / sink / sanitizer for each codeflow step
- optionally include surrounding source code snippets if --repo-root is given

Outputs:
  - <out-dir>/preprocessed_results.jsonl  (one JSON per SARIF result)
  - <out-dir>/preprocessed_results.csv   (flattened summary)

Example usage:
  python3 sarif_preprocessor.py --sarif findings.sarif --out-dir ./out --repo-root /path/to/repo --context 3
"""

import argparse
import json
import csv
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

# Heuristics lists (tweakable)
SOURCE_KEYWORDS = [
    r"request", r"getParameter", r"getInput", r"stdin", r"nextLine", r"readLine", r"argv", r"args", r"env", r"System\.getenv",
    r"fromString", r"Secret", r"password", r"getPassword", r"cookie", r"header", r"queryString", r"untrusted", r"user"
]
SINK_KEYWORDS = [
    r"Runtime\.exec", 
    r"ProcessBuilder",
    r"exec\(",           # escaped (
    r"executeQuery", 
    r"Statement\.execute", 
    r"prepareStatement", 
    r"File\.write",
    r"open\(",           # escaped (
    r"System\.exit", 
    r"Runtime\.getRuntime", 
    r"java/lang/Runtime", 
    r"cmdArgs", 
    r"cmd", 
    r"shell", 
    r"popen", 
    r"ProcessBuilder"
]
SINK_RE = re.compile("|".join(SINK_KEYWORDS), re.IGNORECASE)

SANITIZER_KEYWORDS = [
    r"trim\(", r"replaceAll", r"sanitize", r"escape", r"validate", r"strip", r"clean", r"Pattern", r"Matcher"
]

# Compiled regex sets
SRC_RE = re.compile("|".join(SOURCE_KEYWORDS), re.IGNORECASE)
SINK_RE = re.compile("|".join(SINK_KEYWORDS), re.IGNORECASE)
SAN_RE = re.compile("|".join(SANITIZER_KEYWORDS), re.IGNORECASE)

# Helpful small helpers
def extract_region_from_location(loc: Dict[str, Any]) -> Dict[str, Any]:
    phys = loc.get("physicalLocation") if isinstance(loc, dict) else {}
    art = phys.get("artifactLocation", {}) or {}
    region = phys.get("region", {}) or {}
    return {
        "uri": art.get("uri"),
        "uriBaseId": art.get("uriBaseId"),
        "index": art.get("index"),
        "startLine": region.get("startLine"),
        "endLine": region.get("endLine"),
        "startColumn": region.get("startColumn"),
        "endColumn": region.get("endColumn"),
    }

def classify_step_message(msg: Optional[str]) -> Dict[str, bool]:
    if not msg:
        return {"is_source": False, "is_sink": False, "is_sanitizer": False}
    return {
        "is_source": bool(SRC_RE.search(msg)),
        "is_sink": bool(SINK_RE.search(msg)),
        "is_sanitizer": bool(SAN_RE.search(msg))
    }

def extract_taint_labels(msg: Optional[str]) -> List[str]:
    """
    Pull out small tokens that may indicate taint or type labels, e.g. "apiKey : String" or "fromString(...) : Secret"
    Returns list of "<name>:<type>" patterns or single words that look like taint labels.
    """
    if not msg:
        return []
    labels = []
    # look for "name : Type" patterns
    for m in re.finditer(r"([A-Za-z0-9_./(){}[\] +-]+?)\s*:\s*([A-Za-z0-9_<>]+)", msg):
        left = m.group(1).strip()
        right = m.group(2).strip()
        labels.append(f"{left}:{right}")
    # fallback: find single capitalized tokens like "Secret" or "String"
    for m in re.finditer(r"\b(Secret|String|Integer|int|Password|apiKey|cmdArgs|cmd)\b", msg, re.IGNORECASE):
        labels.append(m.group(1))
    # dedupe preserving order
    seen = set()
    out = []
    for s in labels:
        if s not in seen:
            seen.add(s)
            out.append(s)
    return out

def extract_codeflow_steps(codeflows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out = []
    for cf in codeflows or []:
        for tf in cf.get("threadFlows", []):
            # threadFlows may have multiple locations — they represent a path through program points
            for idx, loc_wrapper in enumerate(tf.get("locations", [])):
                loc = loc_wrapper.get("location", {}) if isinstance(loc_wrapper, dict) else loc_wrapper
                phys = loc.get("physicalLocation", {}) or {}
                art = phys.get("artifactLocation", {}) or {}
                region = phys.get("region", {}) or {}
                msg = (loc.get("message") or {}).get("text") or (loc_wrapper.get("message") or {}).get("text") or None
                step = {
                    "step_index": idx,
                    "file": art.get("uri"),
                    "uriBaseId": art.get("uriBaseId"),
                    "line": region.get("startLine"),
                    "startColumn": region.get("startColumn"),
                    "endColumn": region.get("endColumn"),
                    "message": msg,
                }
                # classification + taint labels
                cls = classify_step_message(msg)
                step.update(cls)
                step["taint_labels"] = extract_taint_labels(msg)
                out.append(step)
    return out

def extract_related(related_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out = []
    for rel in related_list or []:
        phys = rel.get("physicalLocation", {}) or {}
        art = phys.get("artifactLocation", {}) or {}
        region = phys.get("region", {}) or {}
        out.append({
            "id": rel.get("id"),
            "file": art.get("uri"),
            "startLine": region.get("startLine"),
            "startColumn": region.get("startColumn"),
            "message": (rel.get("message") or {}).get("text")
        })
    return out

def read_surrounding_code(repo_root: Path, filename: str, start_line: Optional[int], context: int = 3) -> Optional[Dict[str, Any]]:
    if not repo_root or not filename:
        return None
    path = (repo_root / filename).resolve()
    if not path.exists():
        return None
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as fh:
            lines = fh.readlines()
    except Exception:
        return None
    # adjust for 1-indexing of SARIF lines
    if not start_line:
        return None
    idx = max(0, int(start_line) - 1)
    start = max(0, idx - context)
    end = min(len(lines), idx + context + 1)
    snippet = "".join(lines[start:end])
    return {"path": str(path), "start_line": start+1, "end_line": end, "snippet": snippet}

def flatten_codeflow_for_csv(steps: List[Dict[str, Any]]) -> str:
    parts = []
    for s in steps:
        file = s.get("file") or "<unknown>"
        ln = s.get("line") or ""
        msg = (s.get("message") or "").replace("\n", " ").strip()
        flags = []
        if s.get("is_source"): flags.append("SRC")
        if s.get("is_sink"): flags.append("SINK")
        if s.get("is_sanitizer"): flags.append("SAN")
        parts.append(f"{file}:{ln}:{';'.join(flags)}:{msg}")
    return " | ".join(parts)

def process_sarif(sarif: Dict[str, Any], repo_root: Optional[Path] = None, context: int = 3) -> List[Dict[str, Any]]:
    results = sarif.get("runs", [{}])[0].get("results") if sarif.get("runs") else sarif.get("results") or []
    # normalize: results might be inside runs[0].results per SARIF spec
    processed = []
    for idx, r in enumerate(results or []):
        # main location extraction (first location if present)
        main_loc = None
        if r.get("locations"):
            loc0 = r["locations"][0]
            # some SARIF puts physicalLocation nested differently — handle both
            phys = loc0.get("physicalLocation", loc0.get("physicalLocation", {})) or {}
            art = phys.get("artifactLocation", {}) or {}
            region = phys.get("region", {}) or {}
            main_loc = {
                "uri": art.get("uri"),
                "uriBaseId": art.get("uriBaseId"),
                "startLine": region.get("startLine"),
                "endLine": region.get("endLine"),
                "startColumn": region.get("startColumn"),
                "endColumn": region.get("endColumn")
            }
        partial = r.get("partialFingerprints") or {}
        codeflow_steps = extract_codeflow_steps(r.get("codeFlows", []) or [])
        related = extract_related(r.get("relatedLocations", []) or [])
        # control-flow hints (lightweight): count of alternative threadFlows, and if messages look like conditions
        branch_like = 0
        cond_texts = []
        cf = r.get("codeFlows", []) or []
        for c in cf:
            for tf in c.get("threadFlows", []):
                for loc_wrapper in tf.get("locations", []):
                    msg = None
                    if isinstance(loc_wrapper, dict):
                        msg = (loc_wrapper.get("location") or {}).get("message", {}).get("text") or (loc_wrapper.get("message") or {}).get("text")
                    else:
                        msg = (loc_wrapper.get("message") or {}).get("text")
                    if msg and re.search(r"\b(if|else|switch|case|cond|condition|try|except|catch)\b", msg, re.IGNORECASE):
                        branch_like += 1
                        cond_texts.append(msg)
        # data flow summary: collect taint labels and variable-like tokens from codeflow
        taint_labels = []
        variables = []
        for s in codeflow_steps:
            for lbl in s.get("taint_labels", []):
                if lbl not in taint_labels:
                    taint_labels.append(lbl)
            # variable-like heuristics: words like apiKey, cmdArgs, password, token, input
            if s.get("message"):
                for m in re.finditer(r"\b([A-Za-z_][A-Za-z0-9_]{2,30})\b", s["message"]):
                    tok = m.group(1)
                    if re.search(r"(api|key|pass|token|cmd|arg|input|password|secret)", tok, re.IGNORECASE):
                        if tok not in variables:
                            variables.append(tok)
        # optional code snippets
        snippet = None
        if repo_root and main_loc and main_loc.get("uri") and main_loc.get("startLine"):
            snippet = read_surrounding_code(repo_root, main_loc["uri"], main_loc["startLine"], context=context)
        # assemble record
        record = {
            "result_index": idx,
            "ruleId": r.get("ruleId") or (r.get("rule") or {}).get("id"),
            "ruleIndex": r.get("ruleIndex"),
            "message": (r.get("message") or {}).get("text"),
            "artifact_uri": main_loc.get("uri") if main_loc else None,
            "uriBaseId": main_loc.get("uriBaseId") if main_loc else None,
            "startLine": main_loc.get("startLine") if main_loc else None,
            "endLine": main_loc.get("endLine") if main_loc else None,
            "startColumn": main_loc.get("startColumn") if main_loc else None,
            "endColumn": main_loc.get("endColumn") if main_loc else None,
            "partialFingerprints": partial,
            "primaryLocationLineHash": partial.get("primaryLocationLineHash"),
            "codeflow_steps": codeflow_steps,
            "related_locations": related,
            "control_flow": {
                "branch_like_count": branch_like,
                "branch_messages_sample": cond_texts[:5]
            },
            "data_flow": {
                "taint_labels": taint_labels,
                "variables": variables
            },
            "source_snippet": snippet
        }
        processed.append(record)
    return processed

def write_outputs(records: List[Dict[str, Any]], out_dir: Path):
    out_dir.mkdir(parents=True, exist_ok=True)
    jsonl_path = out_dir / "preprocessed_results.jsonl"
    csv_path = out_dir / "preprocessed_results.csv"
    # JSONL
    with open(jsonl_path, "w", encoding="utf-8") as jf:
        for rec in records:
            jf.write(json.dumps(rec, ensure_ascii=False) + "\n")
    # CSV flattened
    fields = [
        "result_index", "ruleId", "message", "artifact_uri", "startLine", "endLine", "startColumn", "endColumn",
        "primaryLocationLineHash", "partialFingerprints", "codeflow_steps_flat", "related_locations", "branch_like_count",
        "taint_labels", "variables", "snippet_path"
    ]
    with open(csv_path, "w", encoding="utf-8", newline='') as cf:
        writer = csv.DictWriter(cf, fieldnames=fields)
        writer.writeheader()
        for rec in records:
            cf_steps = []
            for s in rec.get("codeflow_steps", []):
                flags = []
                if s.get("is_source"): flags.append("SRC")
                if s.get("is_sink"): flags.append("SINK")
                if s.get("is_sanitizer"): flags.append("SAN")
                cf_steps.append(f"{s.get('file') or ''}:{s.get('line') or ''}:{';'.join(flags)}:{(s.get('message') or '').replace(',', ';')}")
            snippet_path = rec.get("source_snippet", {}).get("path") if rec.get("source_snippet") else None
            writer.writerow({
                "result_index": rec.get("result_index"),
                "ruleId": rec.get("ruleId"),
                "message": (rec.get("message") or "")[:200],
                "artifact_uri": rec.get("artifact_uri"),
                "startLine": rec.get("startLine"),
                "endLine": rec.get("endLine"),
                "startColumn": rec.get("startColumn"),
                "endColumn": rec.get("endColumn"),
                "primaryLocationLineHash": rec.get("primaryLocationLineHash"),
                "partialFingerprints": json.dumps(rec.get("partialFingerprints") or {}),
                "codeflow_steps_flat": " | ".join(cf_steps),
                "related_locations": json.dumps(rec.get("related_locations") or []),
                "branch_like_count": rec.get("control_flow", {}).get("branch_like_count"),
                "taint_labels": json.dumps(rec.get("data_flow", {}).get("taint_labels") or []),
                "variables": json.dumps(rec.get("data_flow", {}).get("variables") or []),
                "snippet_path": snippet_path
            })
    print(f"Wrote JSONL: {jsonl_path}")
    print(f"Wrote CSV:   {csv_path}")

def main():
    p = argparse.ArgumentParser(description="Preprocess SARIF to extract control/data-flow info for Exploiter agent")
    p.add_argument("--sarif", "-s", required=True, help="Path to SARIF file (JSON).")
    p.add_argument("--out-dir", "-o", default="./out", help="Output directory for JSONL/CSV")
    p.add_argument("--repo-root", "-r", default=None, help="Optional path to repo root to fetch surrounding source code")
    p.add_argument("--context", "-c", type=int, default=3, help="Lines of context for source snippets")
    args = p.parse_args()

    sarif_path = Path(args.sarif)
    if not sarif_path.exists():
        print(f"ERROR: SARIF file not found: {sarif_path}")
        return 2
    with open(sarif_path, "r", encoding="utf-8", errors="ignore") as fh:
        sarif = json.load(fh)
    repo_root = Path(args.repo_root) if args.repo_root else None
    records = process_sarif(sarif, repo_root=repo_root, context=args.context)
    write_outputs(records, Path(args.out_dir))
    # print short summary
    print("\nSummary of parsed results:")
    for r in records:
        print(f"- [{r['result_index']}] {r.get('ruleId')}  file={r.get('artifact_uri')}:{r.get('startLine')}  codeflow_steps={len(r.get('codeflow_steps',[]))}  taints={r.get('data_flow', {}).get('taint_labels')} vars={r.get('data_flow', {}).get('variables')}")

if __name__ == '__main__':
    main()
