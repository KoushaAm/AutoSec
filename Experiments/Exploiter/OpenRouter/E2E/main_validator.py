#!/usr/bin/env python3
import os
import re
import json
import time
import argparse
import subprocess
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv

# E2E / AutoSec components
from E2E.control_flow import request_branch_reasoning
from E2E.data_flow import request_flow
from E2E.prompt_gen import build_tester_prompt
from E2E.prompts import BASE_PROMPT
from utils.models import initialize, generate

# ----------------------------------------------------------------------
# Global configuration
# ----------------------------------------------------------------------

time_postfix = time.strftime("%d%m%y_%H%M%S")

DEFAULT_SARIF_SUMMARY_PATH = "E2E/assets/data_preprocessed_perfecto_3.json"
DEFAULT_PROJECT_ROOT = "../jenkinsci__perfecto-plugin_CVE-2020-2261_1.17"

# ----------------------------------------------------------------------
# System-style prompts for validation + auto-fix
# ----------------------------------------------------------------------

JAVA_SYNTAX_VALIDATOR_SYSTEM_PROMPT = """
You are a senior Java engineer and test infrastructure maintainer.

You will be given:
- The root path of a Maven-based Java project.
- A generated JUnit test file (single .java source).
- The stderr output from a javac compilation attempt for this test.

Your goals:
1. Understand why the test does not compile or why it might violate method signatures or types.
2. Produce a corrected version of the SAME test, preserving the intent (same target vulnerability / behaviour),
   but fixing all compilation issues and signature/type mismatches.
3. Produce a corrected version of the SAME test, preserving the intent (same target vulnerability / behaviour),
   but fixing all compilation issues and signature/type mismatches.

   Rules:
- The project is built with Maven; assume dependencies and packages come from its pom.xml plus JUnit.
- Do NOT introduce new external libraries that are not present in the pom.xml unless you rewrite the test
  to rely only on JDK + JUnit.
- Prefer using the project's existing classes and methods; do not invent methods that don’t exist.
- You may adjust imports, method calls, types, and generic parameters to make the test compile.
- Keep the test deterministic and safe: no real shell execution, no network I/O.
- If some target behaviour cannot be accessed directly, mock or wrap the call instead of inventing new APIs.

Output format:
- Return ONLY the corrected Java source file as your entire response.
- Do NOT include markdown fences, explanations, or comments outside the Java code.
"""

JAVA_DEPENDENCY_VALIDATOR_SYSTEM_PROMPT = """
You are a Java build and dependency expert.

You will be given:
- The full content of a Maven pom.xml for a project.
- A generated JUnit test file (.java).
- A list of imports in the test that are considered illegal (because their packages are not supported by pom.xml).

Your task:
1. Rewrite the test so that it uses ONLY:
   - JDK standard library packages (java.*, javax.*),
   - JUnit/JUnit5 packages (org.junit.*, org.junit.jupiter.*),
   - Mockito (org.mockito.*) if desired,
   - And packages that are consistent with dependencies declared in pom.xml.
2. Remove or replace any illegal imports and usages.
3. Preserve the functional intent: the test should still attempt to detect or exploit the described vulnerability,
   using only allowed libraries and the project's own classes.

Rules:
- Do NOT add new third-party libraries not present in pom.xml.
- If some helper library is not available, replace its usage with equivalent logic using standard Java or project code.
- Keep the test deterministic and safe: no real shell execution, no network I/O.

Output format:
- Return ONLY the corrected Java source file as your entire response.
- Do NOT include markdown fences, explanations, or comments outside the Java code.
"""

# ----------------------------------------------------------------------
# Helper functions
# ----------------------------------------------------------------------


def generate_workspace() -> str:
    """Create a timestamped workspace directory for the run."""
    location = f"E2E/assets/run_{time_postfix}/"
    os.makedirs(location, exist_ok=True)
    return location


def clean_llm_java(raw_output: str) -> str:
    """
    Remove markdown fences and leading/trailing whitespace from LLM output
    to get a pure Java source file.
    """
    cleaned = raw_output.strip()

    # remove a leading ``` or ```java
    cleaned = re.sub(r"^```(?:java)?\s*", "", cleaned, flags=re.MULTILINE)

    # remove a trailing ```
    cleaned = re.sub(r"\s*```$", "", cleaned, flags=re.MULTILINE)
    cleaned = cleaned.strip()
    return cleaned


def write_generated_test(java_code: str, workspace: str, filename: Optional[str] = None) -> str:
    """
    Write the generated Java code to a .java file in the workspace.
    Returns the path to the written file.
    """
    if filename is None:
        filename = f"GeneratedExploitTest_{time_postfix}.java"

    test_path = Path(workspace) / filename
    with open(test_path, "w", encoding="utf-8") as f:
        f.write(java_code)
    return str(test_path)


def run_javac_check(test_path: str, project_root: str) -> dict:
    """
    Compile the generated test against the project's test classpath.

    Uses:
      mvn dependency:build-classpath -Dmdep.outputFile=.autosec_test_cp.txt -DincludeScope=test
      javac -cp <classpath> <test_path>

    Returns:
      {
        "ok": bool,
        "stdout": "...",
        "stderr": "...",
        "classpath": "..."
      }
    """
    project_root_path = Path(project_root).resolve()
    cp_file = project_root_path / ".autosec_test_cp.txt"

    # 1) Build classpath via Maven if not cached
    if not cp_file.exists():
        cmd = [
            "mvn",
            "-q",
            "dependency:build-classpath",
            f"-Dmdep.outputFile={cp_file.name}",
            "-DincludeScope=test",
        ]
        res = subprocess.run(
            cmd,
            cwd=str(project_root_path),
            capture_output=True,
            text=True,
        )
        if res.returncode != 0:
            return {
                "ok": False,
                "stdout": res.stdout,
                "stderr": f"Failed to build classpath via Maven:\n{res.stderr}",
                "classpath": "",
            }

    classpath = cp_file.read_text(encoding="utf-8").strip()

    # 2) Compile just the generated test file
    cmd = [
        "javac",
        "-cp",
        classpath,
        test_path,
    ]
    res = subprocess.run(
        cmd,
        cwd=str(project_root_path),
        capture_output=True,
        text=True,
    )

    return {
        "ok": res.returncode == 0,
        "stdout": res.stdout,
        "stderr": res.stderr,
        "classpath": classpath,
    }


def parse_allowed_prefixes_from_pom(project_root: str) -> set[str]:
    """
    Very coarse extraction of allowed package prefixes from pom.xml.

    Always allows:
      	<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>com.pholser</groupId>
			<artifactId>junit-quickcheck-core</artifactId>
			<version>${junit-quickcheck.version}</version>
		</dependency>
		<dependency>
			<groupId>com.pholser</groupId>
			<artifactId>junit-quickcheck-generators</artifactId>
			<version>${junit-quickcheck.version}</version>
		</dependency>
		<dependency>
			<groupId>org.jenkins-ci.plugins</groupId>
			<artifactId>junit</artifactId>
			<version>1.21</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/org.jenkins-ci.plugins/structs -->
		<dependency>
			<groupId>org.jenkins-ci.plugins</groupId>
			<artifactId>structs</artifactId>
			<version>1.7</version>
		</dependency>
		<dependency>
			<groupId>commons-io</groupId>
			<artifactId>commons-io</artifactId>
		</dependency>
		<dependency>
			<groupId>com.auth0</groupId>
			<artifactId>java-jwt</artifactId>
			<version>3.1.0</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/com.google.guava/guava -->
		<dependency>
			<groupId>com.google.guava</groupId>
			<artifactId>guava</artifactId>
		</dependency>
		<dependency>
			<groupId>xml-apis</groupId>
			<artifactId>xml-apis</artifactId>
			<version>1.4.01</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>com.mixpanel</groupId>
			<artifactId>mixpanel-java</artifactId>
			<version>1.4.4</version>
		</dependency>
		<dependency>
			<groupId>org.jenkins-ci.plugins.workflow</groupId>
			<artifactId>workflow-cps</artifactId>
			<version>${plugins.workflow.version}</version>
		</dependency>
		<dependency>
			<groupId>org.jenkins-ci.plugins.workflow</groupId>
			<artifactId>workflow-basic-steps</artifactId>
			<version>2.6</version>
		</dependency>
		<dependency>
			<groupId>org.jenkins-ci.plugins.workflow</groupId>
			<artifactId>workflow-step-api</artifactId>
			<version>2.13</version>
		</dependency>
		<dependency>
			<groupId>org.jenkins-ci.plugins.workflow</groupId>
			<artifactId>workflow-job</artifactId>
			<version>2.15</version>
		</dependency>
		<dependency>
			<groupId>org.jenkins-ci.plugins</groupId>
			<artifactId>credentials</artifactId>
			<version>${plugins.credentials.version}</version>
		</dependency>
		<dependency>
			<groupId>org.jenkins-ci.plugins</groupId>
			<artifactId>matrix-project</artifactId>
			<version>${plugins.matrix-project.version}</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-api -->
		<dependency>
			<groupId>org.slf4j</groupId>
			<artifactId>slf4j-api</artifactId>
			<version>1.7.29</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/commons-net/commons-net -->
		<dependency>
			<groupId>commons-net</groupId>
			<artifactId>commons-net</artifactId>
			<version>3.6</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/org.apache.commons/commons-lang3 -->
		<dependency>
			<groupId>org.apache.commons</groupId>
			<artifactId>commons-lang3</artifactId>
			<version>3.9</version>
		</dependency>
		<dependency>
			<groupId>org.jenkins-ci.plugins</groupId>
			<artifactId>run-condition</artifactId>
			<version>${plugins.run-condition.version}</version>
		</dependency>
		<dependency>
			<groupId>org.json</groupId>
			<artifactId>json</artifactId>
			<version>20200518</version>
			<scope>compile</scope>
		</dependency>
		<dependency>
			<groupId>com.github.stephenc.findbugs</groupId>
			<artifactId>findbugs-annotations</artifactId>
			<version>1.3.9-1</version>
		</dependency>

		<dependency>
			<groupId>org.jenkins-ci</groupId>
			<artifactId>symbol-annotation</artifactId>
			<version>1.10</version>
		</dependency>
		<dependency>
			<groupId>org.apache.ant</groupId>
			<artifactId>ant</artifactId>
		</dependency>
		<dependency>
			<groupId>org.codehaus.plexus</groupId>
			<artifactId>plexus-classworlds</artifactId>
			<version>2.6.0</version>
		</dependency>
		<dependency>
			<groupId>org.codehaus.plexus</groupId>
			<artifactId>plexus-utils</artifactId>
			<version>3.3.0</version>
		</dependency>
		<dependency>
			<groupId>org.apache.httpcomponents</groupId>
			<artifactId>httpclient</artifactId>
			<version>4.5.12</version>
		</dependency>
		<dependency>
			<groupId>org.apache.httpcomponents</groupId>
			<artifactId>httpcore</artifactId>
			<version>4.4.13</version>
		</dependency>
		<dependency>
			<groupId>commons-codec</groupId>
			<artifactId>commons-codec</artifactId>
		</dependency>
		<dependency>
			<groupId>org.eclipse.sisu</groupId>
			<artifactId>org.eclipse.sisu.plexus</artifactId>
			<version>0.3.4</version>
		</dependency>
		<dependency>
			<groupId>org.codehaus.plexus</groupId>
			<artifactId>plexus-interpolation</artifactId>
			<version>1.26</version>
		</dependency>
		<dependency>
			<groupId>com.fasterxml.jackson.core</groupId>
			<artifactId>jackson-databind</artifactId>
			<version>2.11.0</version>
		</dependency>
		<dependency>
			<groupId>org.hamcrest</groupId>
			<artifactId>hamcrest-core</artifactId>
			<version>2.2</version>
			<scope>test</scope>
		</dependency>


    Additionally, for each dependency groupId, we add a prefix up to 3 segments, e.g.:
      org.apache.commons -> org.apache.commons.
      com.fasterxml.jackson.core -> com.fasterxml.jackson.
    """
    pom_path = Path(project_root) / "pom.xml"
    prefixes: set[str] = {
        "java.",
        "javax.",
        "org.junit.",
        "org.junit.jupiter.",
        "org.mockito.",
    }

    if not pom_path.exists():
        return prefixes

    # Parse pom.xml with namespace handling
    tree = ET.parse(str(pom_path))
    root = tree.getroot()
    ns_uri = root.tag.split("}")[0].strip("{")
    ns = {"m": ns_uri} if ns_uri else {}

    for dep in root.findall(".//m:dependency", ns):
        group_id_el = dep.find("m:groupId", ns)
        if group_id_el is None or not group_id_el.text:
            continue
        group_id = group_id_el.text.strip()
        if not group_id:
            continue

        parts = group_id.split(".")
        if len(parts) >= 2:
            # up to 3 segments for safety
            prefix = ".".join(parts[: min(3, len(parts))]) + "."
        else:
            prefix = group_id + "."
        prefixes.add(prefix)

    return prefixes


IMPORT_RE = re.compile(r"^\s*import\s+([a-zA-Z0-9_.]+);", re.MULTILINE)


def check_imports_against_pom(test_path: str, project_root: str) -> dict:
    """
    Check whether imports in the given Java test are compatible with the
    project's pom.xml dependencies.

    Returns:
      {
        "ok": bool,
        "illegal_imports": [ ... ],
        "allowed_prefixes": [ ... ],
        "all_imports": [ ... ]
      }
    """
    code = Path(test_path).read_text(encoding="utf-8")
    imports = IMPORT_RE.findall(code)
    allowed_prefixes = parse_allowed_prefixes_from_pom(project_root)

    illegal: list[str] = []
    for imp in imports:
        if not any(imp.startswith(pref) for pref in allowed_prefixes):
            illegal.append(imp)

    return {
        "ok": len(illegal) == 0,
        "illegal_imports": illegal,
        "allowed_prefixes": sorted(allowed_prefixes),
        "all_imports": imports,
    }


def llm_fix_syntax(
    client,
    model,
    original_java: str,
    javac_result: dict,
    project_root: str,
     target_source: str,
) -> str:
    """
    Use the LLM to fix syntax/signature/type errors based on javac output.

    Returns the (raw) LLM response, which should be Java code; caller should
    run clean_llm_java() before writing/compiling.
    """
    prompt = f"""{JAVA_SYNTAX_VALIDATOR_SYSTEM_PROMPT}

<PROJECT_ROOT>
{project_root}
</PROJECT_ROOT>

<TARGET_CLASS_SOURCE>
{target_source}
</TARGET_CLASS_SOURCE>

<JAVAC_STDERR>
{javac_result.get("stderr","").strip()}
</JAVAC_STDERR>

<ORIGINAL_TEST_JAVA>
{original_java}
</ORIGINAL_TEST_JAVA>
"""
    return generate(client, model, prompt)


def llm_fix_dependencies(
    client,
    model,
    original_java: str,
    illegal_imports: list[str],
    pom_xml: str,
) -> str:
    """
    Use the LLM to fix illegal imports/usage based on pom.xml.

    Returns the (raw) LLM response, which should be Java code; caller should
    run clean_llm_java() before writing/compiling.
    """
    illegal_list = "\n".join(f"- {imp}" for imp in illegal_imports)
    prompt = f"""{JAVA_DEPENDENCY_VALIDATOR_SYSTEM_PROMPT}

<ILLEGAL_IMPORTS>
{illegal_list}
</ILLEGAL_IMPORTS>

<POM_XML>
{pom_xml}
</POM_XML>

<ORIGINAL_TEST_JAVA>
{original_java}
</ORIGINAL_TEST_JAVA>
"""
    return generate(client, model, prompt)


from pathlib import Path

def load_perfecto_wrapper_source(project_root: str) -> str:
    """
    Load the PerfectoBuildWrapper.java source so the LLM can inspect
    real method signatures and fields.
    """
    src_path = Path(project_root) / "src/main/java/io/plugins/perfecto/PerfectoBuildWrapper.java"
    return src_path.read_text(encoding="utf-8")


# ----------------------------------------------------------------------
# Main pipeline
# ----------------------------------------------------------------------


def pipeline(project_root: str, sarif_summary_path: str) -> None:
    workspace = generate_workspace()
    print(f"[*] Workspace: {workspace}")

    print("[*] Loading environment and initializing model client...")
    load_dotenv()
    client, model = initialize()

    print(f"[*] Reading SARIF summary from: {sarif_summary_path}")
    sarif_summary = Path(sarif_summary_path).read_text(encoding="utf-8")

    perfecto_source = load_perfecto_wrapper_source(project_root)
    # ------------------------------------------------------------------
    # 1. Data Flow Analysis
    # ------------------------------------------------------------------
    print("[*] Running Data Flow Analysis...")
    flow_result = request_flow(
        client,
        model,
        BASE_PROMPT,
        project_root,
        sarif_summary,
    )
    flow_json = flow_result.get("flow", [])
    with open(f"{workspace}data_flow_{time_postfix}.json", "w", encoding="utf-8") as f:
        json.dump(flow_result, f, indent=2)

    # Normalize the flow into a list of dicts expected by control_flow.collect_branch_evidence
    normalized_flow = []
    try:
        if isinstance(flow_json, dict):
            # common case: flow_json may itself contain a 'flow' key or be a mapping
            if isinstance(flow_json.get("flow"), list):
                cand = flow_json.get("flow")
            else:
                cand = list(flow_json.values())
        elif isinstance(flow_json, list):
            cand = flow_json
        else:
            cand = []

        for item in cand:
            if isinstance(item, dict):
                normalized_flow.append(item)
            elif isinstance(item, str):
                # try to parse JSON text if LLM returned JSON as strings
                try:
                    parsed = json.loads(item)
                    if isinstance(parsed, dict):
                        normalized_flow.append(parsed)
                    elif isinstance(parsed, list):
                        for it in parsed:
                            if isinstance(it, dict):
                                normalized_flow.append(it)
                except Exception:
                    # heuristics: if string looks like a path, convert to minimal node
                    if ".java" in item or "/" in item or "\\" in item:
                        normalized_flow.append({"file": item, "start_line": 1})
                    else:
                        # ignore other free-form strings
                        continue
    except Exception:
        normalized_flow = []

    # fallback: if normalization produced nothing, use original to preserve behavior
    if not normalized_flow:
        normalized_flow = flow_json if isinstance(flow_json, list) else [flow_json]

    # ------------------------------------------------------------------
    # 2. Control Flow Analysis
    # ------------------------------------------------------------------
    print("[*] Running Control Flow Analysis...")
    control_result = request_branch_reasoning(
        client,
        model,
        BASE_PROMPT,
        project_root,
        flow_json,
        sarif_summary,
    )
    with open(f"{workspace}control_flow_{time_postfix}.json", "w", encoding="utf-8") as f:
        json.dump(control_result, f, indent=2)

    control_branches = control_result.get("branches", control_result)

    # ------------------------------------------------------------------
    # 3. Tester Prompt Generation + Exploiter Call
    # ------------------------------------------------------------------
    print("[*] Building Tester Prompt...")
    tester_prompt = build_tester_prompt(
        data_flow=flow_json,
        control_flow=control_branches,
        sarif_summary=sarif_summary,
        project_root=project_root,
        sarif_summary_path=sarif_summary_path,
    )

    print("[*] Calling Exploiter model to generate test...")
    raw_output = generate(client, model, tester_prompt)
    raw_output_path = f"{workspace}tester_raw_output_{time_postfix}.txt"
    with open(raw_output_path, "w", encoding="utf-8") as fh:
        fh.write(raw_output)
    print(f"[+] Raw tester output written to: {raw_output_path}")

    # Clean up code fences → pure Java
    java_code = clean_llm_java(raw_output)
    test_path = write_generated_test(java_code, workspace)
    print(f"[+] Initial generated test written to: {test_path}")

    # ------------------------------------------------------------------
    # 4. Check 1: Compilation (Signatures / Types)
    # ------------------------------------------------------------------
    print("[*] Running javac compilation check...")
    javac_result = run_javac_check(test_path, project_root)
    with open(f"{workspace}javac_result_initial_{time_postfix}.json", "w", encoding="utf-8") as f:
        json.dump(javac_result, f, indent=2)

    if not javac_result["ok"]:
        print("[!] javac reported errors. Attempting LLM-based syntax fix...")
        fixed_raw = llm_fix_syntax(client, model, java_code, javac_result, project_root, perfecto_source)
        fixed_clean = clean_llm_java(fixed_raw)
        test_path = write_generated_test(fixed_clean, workspace, filename=f"GeneratedExploitTest_fixed1_{time_postfix}.java")

        javac_result2 = run_javac_check(test_path, project_root)
        with open(f"{workspace}javac_result_fixed1_{time_postfix}.json", "w", encoding="utf-8") as f:
            json.dump(javac_result2, f, indent=2)

        if not javac_result2["ok"]:
            print("[!] javac still failing after syntax fix. See JSON logs for details.")
        else:
            print("[+] javac compilation succeeded after syntax fix.")

    else:
        print("[+] javac compilation succeeded on first attempt.")

    # ------------------------------------------------------------------
    # 5. Check 2: Dependency / Import Compatibility
    # ------------------------------------------------------------------
    print("[*] Running dependency/import compatibility check against pom.xml...")
    dep_check = check_imports_against_pom(test_path, project_root)
    with open(f"{workspace}dependency_check_initial_{time_postfix}.json", "w", encoding="utf-8") as f:
        json.dump(dep_check, f, indent=2)

    if not dep_check["ok"]:
        print("[!] Found illegal imports in generated test:")
        for imp in dep_check["illegal_imports"]:
            print(f"    - {imp}")

        pom_path = Path(project_root) / "pom.xml"
        pom_xml = pom_path.read_text(encoding="utf-8") if pom_path.exists() else ""

        original_java = Path(test_path).read_text(encoding="utf-8")
        fixed_dep_raw = llm_fix_dependencies(
            client,
            model,
            original_java,
            dep_check["illegal_imports"],
            pom_xml,
        )
        fixed_dep_clean = clean_llm_java(fixed_dep_raw)
        test_path = write_generated_test(fixed_dep_clean, workspace, filename=f"GeneratedExploitTest_fixed_deps_{time_postfix}.java")

        dep_check2 = check_imports_against_pom(test_path, project_root)
        with open(f"{workspace}dependency_check_fixed_{time_postfix}.json", "w", encoding="utf-8") as f:
            json.dump(dep_check2, f, indent=2)

        if not dep_check2["ok"]:
            print("[!] Test still uses illegal imports after dependency fix. See JSON logs.")
        else:
            print("[+] Dependency/import check passed after fix.")
    else:
        print("[+] Dependency/import check passed on first attempt.")

    print(f"[✓] Pipeline complete. Final test file: {test_path}")


# ----------------------------------------------------------------------
# Entrypoint
# ----------------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AutoSec E2E Exploiter Pipeline with Script Validation")
    parser.add_argument(
        "--project",
        type=str,
        required=False,
        default=DEFAULT_PROJECT_ROOT,
        help="Path to the project root",
    )
    parser.add_argument(
        "--sarif",
        type=str,
        required=False,
        default=DEFAULT_SARIF_SUMMARY_PATH,
        help="Path to the preprocessed SARIF summary JSON/text file",
    )

    args = parser.parse_args()
    pipeline(project_root=args.project, sarif_summary_path=args.sarif)
