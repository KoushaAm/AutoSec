import requests
import os 
from dotenv import load_dotenv
from utils.models import *
from openai import OpenAI
from utils.prompts.system_prompts import *
# from utils.prompts.analyzer_prompt import *
import json

SARIF_SUMMARY_PATH = "Analyzer/data_preprocessed.json"
CODE_PATH = "project_cmd_2/Main.java"

def initialize():
    load_dotenv()
    model = LLAMA3 # model identifier for openrouter
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key= os.getenv("OPEN_ROUTER_API_KEY")
    )

    return client, model

def generate(client, model, prompt) : 
    completion = client.chat.completions.create(
  
    model=model,
    messages=[
        {
        "role": "user",
        "content": prompt
        }
    ]
    )

    return completion.choices[0].message.content


sarif_data = read_sarif_summary(SARIF_SUMMARY_PATH)
code_data = read_source_code(CODE_PATH)

# ensure both are strings
if not isinstance(sarif_data, str):
    sarif_text = json.dumps(sarif_data, indent=2)
else:
    sarif_text = sarif_data

if not isinstance(code_data, str):
    code_text = json.dumps(code_data, indent=2)
else:
    code_text = code_data


prompt = analyzer_prompt(code_text, sarif_text)

# print(prompt)
client, model = initialize()

output = generate(client, model, prompt)
print("Generated Analysis Output:")
print(output)

# Save the output to a file
output_path = "Analyzer/analysis_output.txt"
with open(output_path, "w", encoding="utf-8") as f:
    f.write(output)

print(f"\nOutput successfully saved to {output_path}")