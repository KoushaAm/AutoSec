from vuln_agent.prompts import *
from vuln_agent.tools import *
from vuln_agent.helpers import *
from vuln_agent.conversation import Conversation

class Run(Tool):

    def __init__(self, dataset, project_name, workdir, logger):
        """
        initializes the Run tool.
        This tool builds and runs the docker image for the project.
        """
        self.dataset = dataset
        self.project_name = project_name
        self.workdir = workdir
        self.logger = logger

    def get_name(self):
        return "run"

    def get_description(self):
        return "Builds and runs the docker image for the project."

    def get_usage(self):
        return ('<TOOL>\n'
                '{"name": "run"}\n'
                '</TOOL>\n')

    def _docker_build(self, platform=None, context_root="."):
        plat = f"--platform={platform} " if platform else ""
        return run(
            f"docker build {plat}-f ./Dockerfile.vuln -t {self.project_name.lower()}_vuln {context_root}",
            timeout=300,
            logger=self.logger
        )

    def execute(self, param_dict: str):
        CAUTION_MSG=f"""Carefully analyze this output for errors or messages that can help you debug your test.
        If it is not the behavior you expected:
        1. Step back and reflect on 5-7 different possible sources of the problem
        2. Assess the likelihood of each possible cause
        3. Methodically address the most likely causes, starting with the highest probability
        4. If necessary, add print statements to the source code to debug the issue
        
        If you are having issues with Docker "refsums", remember that you don't need to add any new COPY commands in the Dockerfile.
        If your Docker build is timing out, try using the Reset tool to reset the working directory and start from scratch.
        
        Lastly, remember that your test should actually run the vulnerable code in the project.
        - It should NOT read the source code to check for the presence of a vulnerability.
        - It should NOT \"simulate\" the vulnerability by running some separate code that does not use the project.
        - Make sure that the test file is inside the test folder of the src folder
        """
        os.chdir(self.workdir)
        # context_root = "../.." if self.dataset == 'cwe-bench-java' else "."
        # Check if there are other keys in the param_dict
        for key in param_dict.keys():
            if key not in ["name"]:
                return {"status": "Failure", "output": f"Unknown field '{key}'"}

        try:
            self._docker_build()
        except RunException as e:
            msg = str(e)
            if "no match for platform in manifest" in msg:
                self.logger.log_status("Retrying docker build with linux/amd64 due to manifest mismatch...")
                try:
                    self._docker_build(platform="linux/amd64")
                except RunException as e2:
                    return {"status": "Failure",
                            "output": f"Build failed (even after amd64 retry): {truncate_reverse(str(e2), 10000)}\n{CAUTION_MSG}"}
            else:
                return {"status": "Failure", "output": f"Build failed: {truncate_reverse(msg, 10000)}\n{CAUTION_MSG}"}

        # using a helper (docker_build) now this is why this is commented
        # try:
        #     run(f"docker build -f ./Dockerfile.vuln -t {self.project_name.lower()}_vuln {context_root}",
        #         timeout=300,
        #         logger=self.logger)
        # except RunException as e:
        #     return {"status": "Success", "output": f"Build failed: {truncate_reverse(str(e), 10000)}\n{CAUTION_MSG}"}

        self.logger.log_status("Docker image built successfully.")

        try:
            stdout = run(f"docker run --rm {self.project_name.lower()}_vuln",
                timeout=200,
                logger=self.logger)
            return {"status": "Success", "output": f"Run succeeded. STDOUT:\n{truncate_reverse(stdout, 10000)}\n{CAUTION_MSG}"}
        except RunException as e:
            return {"status": "Failure", "output": f"Run exited with non-zero code.\n{truncate_reverse(str(e), 10000)}\n{CAUTION_MSG}"}

class Reset(Tool):

    def __init__(self, workdir, logger):
        """
        Initializes the Reset tool.
        This tool resets the working directory to the initial state.
        """
        self.workdir = workdir
        self.logger = logger

    def get_name(self):
        return "reset"

    def get_description(self):
        return "Resets the working directory to the initial state."

    def get_usage(self):
        return ('<TOOL>\n'
                '{"name": "reset"}\n'
                '</TOOL>\n')

    def execute(self, param_dict: str):
        files_to_preserve = [
            ".build_diff.patch",
            ".Dockerfile.backup",
            "Dockerfile.vuln",
            "autosec"
        ]

        # Check if there are other keys in the param_dict
        for key in param_dict.keys():
            if key not in ["name"]:
                return {"status": "Failure", "output": f"Unknown field '{key}'"}

        os.chdir(self.workdir)
        try:
            run("git stash", logger=self.logger)
            result = run("git ls-files --others --exclude-standard", logger=self.logger)
            created_files = result.strip().splitlines()
            created_files = [f for f in created_files if f.strip() not in files_to_preserve]
        except RunException as e:
            return {"status": "Failure", "output": f"Reset failed."}

        for f in created_files:
            p = Path(self.workdir) / f
            if p.is_dir():
                shutil.rmtree(p)
            else:
                p.unlink()

        # Restore Dockerfile from backup
        dockerfile_backup = self.workdir / ".Dockerfile.backup"
        if dockerfile_backup.exists():
            if (self.workdir / "Dockerfile.vuln").exists():
                Path(self.workdir / "Dockerfile.vuln").unlink()
            shutil.copy(dockerfile_backup, self.workdir / "Dockerfile.vuln")
            
        return {"status": "Success", "output": "Working directory reset successfully."}


class TestGen:
    def __init__(self, model, dataset, project_name, workdir, logger, init_conversation, flow, conditions, max_turns=50):
        self.model = model
        self.dataset = dataset
        self.project_name = project_name
        self.workdir = workdir
        self.logger = logger
        self.max_turns = max_turns
        self.conversation = init_conversation
        self.flow = flow
        self.conditions = conditions

        self.tools = [tool_class(self.logger) for tool_class in [ListDir, Read, Grep, Find, Write, Mkdir]]
        self.tools += [Run(dataset, project_name, workdir, logger), Reset(workdir, logger)]
        self.tool_manager = Tooling(self.logger)
        for tool in self.tools:
            self.tool_manager.register_tool(tool)

    def get_conversation(self):
        return self.conversation

    def get_issue_details(self):

        if self.dataset == 'cwe-bench-java':
            advisory_path = Path(self.workdir) / "../../../advisory" / f"{self.project_name}.json"
            if not advisory_path.exists():
                self.logger.log_failure(f"Advisory file {advisory_path} does not exist.")
                return None, None
            with open(advisory_path, 'r') as f:
                advisory_data = json.load(f)
            if 'details' not in advisory_data:
                self.logger.log_failure(f"No details found in advisory file {advisory_path}.")
            if 'summary' not in advisory_data:
                self.logger.log_failure(f"No summary found in advisory file {advisory_path}.")
            cwe_ids = advisory_data["database_specific"]["cwe_ids"]
            issue_desc = advisory_data['details'] if 'details' in advisory_data else None
            issue_summary = advisory_data['summary'] if 'summary' in advisory_data else None
            return cwe_ids, issue_desc, issue_summary

        elif self.dataset == 'primevul':
            info_path = Path(self.workdir) / "../../../processed_info.json"
            if not info_path.exists():
                self.logger.log_failure(f"Processed info file {info_path} does not exist.")
                return None, None, None
            with open(info_path, 'r') as f:
                processed_info = json.load(f)
            if self.project_name not in processed_info:
                self.logger.log_failure(f"No information found for project {self.project_name} in {info_path}.")
                return None, None, None
            project_info = processed_info[self.project_name]
            cwe_ids = project_info['cwe_ids']
            issue_desc = project_info['cve_desc'] if 'cve_desc' in project_info else None
            return cwe_ids, issue_desc, None
        else:
            raise ValueError(f"Unsupported dataset {self.dataset}. Supported datasets are: ['cwe-bench-java', 'primevul']")

    def run(self, max_tool_calls=20, finalize_last_n_turns=2):
        """
        Run the test generation module.
        """
        max_tool_calls = self.max_turns - 7
        self.logger.log_status("Running test generation module...")

        cwe_ids, issue_desc, issue_summary = self.get_issue_details()
        if not issue_desc:
            self.logger.log_failure("Failed to retrieve issue details.")
            return

        cwe_to_instruction = {
            "CWE-22": ("This is a Path Traversal vulnerability (CWE-22). "
                       "The test case must call an externally accessible API of the project with appropriate inputs, such that "
                       "it reads from or writes to at least one file outside the project directory."),
            "CWE-78": ("This is a Command Injection vulnerability (CWE-78). "
                       "The test case must call an externally accessible API of the project with appropriate inputs, such that "
                       "it executes a shell command that is not intended by the application."),
            "CWE-79": ("This is a Cross-Site Scripting (XSS) vulnerability (CWE-79). "
                       "The test case must call an externally accessible API of the project "
                       "with an input that contains scripting code, and show that this input is not santized properly."),
            "CWE-94": ("This is a Code Injection vulnerability (CWE-94). "
                       "The test case must call an externally accessible API of the project with appropriate inputs, such that "
                       "it executes some code that is not intended by the application."),
        }

        cwe_desc = None
        for cwe in cwe_ids:
            if cwe in cwe_to_instruction:
                cwe_desc = cwe_to_instruction[cwe]
                break
        if cwe_desc is None:
            raise ValueError(
                f"Unsupported CWE ID(s) {cwe_ids} for project {self.project_name}. "
                f"Supported CWE IDs are: {list(cwe_to_instruction.keys())}"
            )

        docker_instructions = construct_docker_instructions(self.dataset, self.workdir)

        # Construct the prompt
        prompt = construct_issue_desc_prompt(issue_desc, issue_summary, diff=None)
        prompt += f"""
    Now create a test case that FAILS (exits with non-zero code) if the vulnerability EXISTS,
    and PASSES (exits with code 0) if the vulnerability DOES NOT EXIST.
    {cwe_desc}

    This test should actually run the vulnerable code in the project.
    - It should NOT read the source code to check for the presence of a vulnerability.
    - It should NOT "simulate" the vulnerability by running some separate code that does not use the project.

    {f"Here is a flow consisting of a sequence of program points to reach the vulnerability:\\n{self.flow}" if self.flow else ""}

    The test should start from the vulnerability 'source' and reach the 'sink'.
    It should be designed such that it passes through all the branch conditions on the way.
    {f"This means that the input and method calls should be carefully crafted, satisfying the following conditions:\\n{self.conditions}" if self.conditions else ""}

    {docker_instructions}

    Feel free to create any new files to create the test case.
    You are highly encouraged to insert print statements in the existing source files to debug your test.

    Once you verify that the flow has reached the 'sink', you should analyze the observed behavior of the program
    to ensure that the test FAILS if the vulnerability exists, and PASSES if it does not exist.

    ===== DOCKER RUN RULES =====
    1) You must write a POV test file before building and running Docker for test.

    ===== TEST PLACEMENT RULES (MANDATORY) =====
    1) You MUST write the PoV test ONLY under: autosec/tests/... (assume autosec/tests exists)
       e.g. autosec/tests/<RELATIVE_PATH_FROM_REPO_ROOT>
    2) The path inside autosec/tests MUST mirror the repo path where Maven expects tests.
    3) DO NOT write directly into any moduleâ€™s src/test/java (validation will materialize it).
    4) The test class name MUST end with Test (e.g., CWE22_nameTest) so Maven Surefire detects it.
    5) Use the project's existing test framework.

    ===== PoV MARKER RULE (MANDATORY) =====
    When the vulnerability is successfully triggered (i.e., the test should FAIL),
    you MUST fail the test with an assertion that includes this exact marker string:
    AUTOSEC_POV_TRIGGERED

    Also print the marker to stdout right before failing:
      System.out.println("AUTOSEC_POV_TRIGGERED");

    IMPORTANT STOP RULE:
    - If you ever see AUTOSEC_POV_TRIGGERED in output, the PoV succeeded.
    - Do NOT modify the test after that.
    - Immediately respond with <DONE>.

    ===== END RULES =====
    """

        prompt += construct_tool_prompt(self.tools)
        prompt += (
            "If you successfully generate the test case and confirm that it satisfies all the above conditions, "
            "respond with a message that contains the string \"<DONE>\".\n"
            "IMPORTANT: You have a limited number of turns and tool calls. "
            "By the last few turns, you MUST have written at least one test file and run it using the Run tool. "
            "After that, you should stop calling tools and simply reply with <DONE>.\n"
        )

        self.conversation.add_message("user", prompt)
        self.logger.log_output(prompt)

        # Track progress
        test_written = False
        test_ran = False
        tool_calls = 0

        # NEW: hard success latch
        pov_triggered = False
        POV_MARKER = "AUTOSEC_POV_TRIGGERED"

        for turn in range(self.max_turns):
            response = self.conversation.generate()
            self.logger.log_output(response)

            in_final_window = turn >= self.max_turns - finalize_last_n_turns

            # If assistant signals DONE at any point, accept it
            if "<DONE>" in response:
                self.logger.log_status("Test generation completed (assistant signaled <DONE>).")
                break

            if self.tool_manager.has_tool_invocation(response):
                self.logger.log_status("Tool invocation detected.")

                # If we've already succeeded, stop immediately (don't let it rewrite)
                if pov_triggered:
                    self.logger.log_status("PoV marker already observed; stopping tool execution and finalizing.")
                    self.conversation.add_message(
                        "user",
                        "AUTOSEC_POV_TRIGGERED was already observed. Do NOT modify any files. Reply with <DONE> now."
                    )
                    break

                # Tool limit / finalization rule
                if tool_calls >= max_tool_calls or (in_final_window and test_written and test_ran):
                    self.logger.log_status(
                        "Tool limit reached or in finalization window with test ready; forcing <DONE>."
                    )
                    final_msg = (
                        "You have already used tools sufficiently. Do NOT invoke any more tools. "
                        "If the current test satisfies requirements, reply now with <DONE> and nothing else."
                    )
                    self.conversation.add_message("user", final_msg)
                    self.logger.log_status(final_msg)
                    continue

                # Execute tool
                tool_output = self.tool_manager.invoke_tool(response)
                self.logger.log_output(tool_output)
                tool_calls += 1

                # NEW: latch PoV success as soon as marker appears in ANY tool output (Success or Failure)
                out_text_any = tool_output.get("output", "") if isinstance(tool_output, dict) else str(tool_output)
                ran_docker = ("Run succeeded." in out_text_any) or ("Run exited with non-zero code." in out_text_any)

                # out_text_any = tool_output.get("output", "") if isinstance(tool_output, dict) else str(tool_output)
                ran_docker = ("Run succeeded." in out_text_any) or ("Run exited with non-zero code." in out_text_any)

                if ran_docker and (POV_MARKER in out_text_any):
                    pov_triggered = True
                    self.logger.log_status("PoV marker observed in tool output; forcing stop and finalizing.")

                    # not need we can save tokens by just returning success
                    # self.conversation.add_message("user", out_text_any)
                    # stop_msg = (
                    #     "AUTOSEC_POV_TRIGGERED was observed in the output. "
                    #     "This means the PoV is correct for the vulnerable version. "
                    #     "Do NOT modify any files. Reply with <DONE> now."
                    # )
                    # self.conversation.add_message("user", stop_msg)
                    # self.logger.log_status(stop_msg)

                    return "Success"

                if tool_output.get("status") == "Success":
                    out_text = tool_output.get("output", "")

                    # Heuristic: mark test_written when Write tool succeeds
                    if "File written successfully" in out_text:
                        test_written = True

                    # Heuristic: mark test_ran when Run tool executes
                    if "Run succeeded." in out_text or "Run exited with non-zero code." in out_text:
                        test_ran = True

                    # NEW: if marker appears anywhere, latch success and force stop
                    ran_docker = ("Run succeeded." in out_text) or ("Run exited with non-zero code." in out_text)

                    if ran_docker and (POV_MARKER in out_text):
                        pov_triggered = True
                        self.conversation.add_message("user", out_text)
                        self.logger.log_status(out_text)

                        stop_msg = (
                            "AUTOSEC_POV_TRIGGERED was observed in the output. "
                            "This means the PoV is correct for the vulnerable version. "
                            "Do NOT modify any files. Reply with <DONE> now."
                        )
                        self.conversation.add_message("user", stop_msg)
                        self.logger.log_status(stop_msg)
                        break  # <<< IMPORTANT: do not allow further tool calls / rewrites

                    # Normal path: keep going
                    self.conversation.add_message("user", out_text)
                    self.logger.log_status(out_text)

                    if "File written successfully" in out_text:
                        hint = "If you have finished generating your test, use the Run tool to check it."
                        self.conversation.add_message("user", hint)
                        self.logger.log_status(hint)
                else:
                    msg = f"Tool invocation failed: {tool_output.get('output', '')}"
                    self.conversation.add_message("user", msg)
                    self.logger.log_status(msg)

            else:
                # no tool and no <DONE>: push the model
                if pov_triggered:
                    self.conversation.add_message(
                        "user",
                        "AUTOSEC_POV_TRIGGERED was already observed. Do NOT modify anything. Reply with <DONE>."
                    )
                    break

                if in_final_window:
                    final_msg_parts = []
                    if not test_written:
                        final_msg_parts.append(
                            "You have not yet written a concrete test file. You MUST now create the test using the Write tool."
                        )
                    if not test_ran:
                        final_msg_parts.append(
                            "You have not yet run the test. You MUST call the Run tool at least once to execute the test."
                        )
                    if test_written and test_ran:
                        final_msg_parts.append(
                            "You have already written and run the test. Do NOT invoke any more tools. Reply with <DONE>."
                        )
                    else:
                        final_msg_parts.append(
                            "After you have written and run the test, reply with <DONE> and DO NOT invoke any more tools."
                        )
                    continue_message = " ".join(final_msg_parts)
                else:
                    continue_message = (
                        "Your output doesn't contain a <TOOL>...</TOOL> invocation or <DONE>. "
                        "Use the tools (especially Write and Run) to create and run the test. "
                        "When you have generated, run and checked your test, respond with <DONE>."
                    )

                self.conversation.add_message("user", continue_message)
                self.logger.log_status(continue_message)

        if len(self.conversation.messages) == 0 or self.conversation.messages[-1]["role"] != "assistant":
            self.logger.log_failure("Test generation failed to produce a valid response.")
            return "Failure"

        if "<DONE>" in self.conversation.messages[-1]["content"] or pov_triggered:
            self.logger.log_success("Test generation completed successfully.")
            return "Success"
        else:
            self.logger.log_failure("Test generation failed to produce a valid test case.")
            return "Failure"


    def repair(self, feedback):
        """
        Repair the test case based on feedback.
        """
        if len(self.conversation.messages) == 0:
            self.logger.log_failure("No conversation history to repair.")
            return

        prompt = (
            "The test you generated had the following error:\n"
            f"{feedback}\n"
            "Please fix the test case. Carefully analyze this output for errors or messages that can help you debug your test. "
            "Reason step-by-step about what might have gone wrong, and how you can fix it.\n"
            "You can use the <TOOL>...</TOOL> format to invoke tools, and you can also add new files.\n"
            "When you have generated, run and checked your test again, respond with a message containing the string \"<DONE>\".\n"
            "Remember that the test should actually run the vulnerable code in the project, "
            "- It should NOT read the source code to check for the presence of a vulnerability.\n"
            "- It should NOT \"simulate\" the vulnerability by running some separate code that does not use the project.\n"
            f"""
            #### DOCKER RULES ####
            Rules:
                - Do NOT modify any lines above: "# Do not modify anything above this line" (if present).
                - The entire project directory is copied into the container already; do NOT add new COPY commands.
                - The test must be executed via the Dockerfile CMD so it runs with: `docker run --rm <image>`.
                - Sometimes the POV test you generated is not found by the Docker run command inside Dockerfile.vuln make sure that test file is found and addressed properly.
                
                Important robustness requirements:
                - Do NOT assume the Maven/Gradle project root. Before finalizing CMD/WORKDIR, locate build files:
                  - The original pom.xml should exist in the project root {self.workdir} make sure you use that for building proper environment.
                  - Maven: find `/usr/src/app` for `pom.xml`
                  - Gradle: find `/usr/src/app` for `build.gradle` or `gradlew`
                - If `mvn test` fails with "no POM in this directory" / MissingProjectException:
                  - locate the correct pom.xml and use either:
                    - `WORKDIR` to that directory, OR
                    - `mvn -f <path-to-pom.xml> test` in CMD
                
                Platform note:
                - Some old Maven base images do not support arm64. If you hit "no match for platform in manifest",
                  use a base image that installs Java/Maven 
                
                MUST NOT run tests in RUN steps; tests go in CMD.
    
                MUST discover pom location (either via provided best_pom OR via find in CMD).
                            
                MUST NOT add COPY lines (if harness already copies).
            """

        )

        self.conversation.add_message("user", prompt)
        self.logger.log_output(prompt)

        for turn in range(self.max_turns):
            response = self.conversation.generate()
            self.logger.log_output(response)
            if self.tool_manager.has_tool_invocation(response):
                self.logger.log_status("Tool invocation detected.")
                tool_output = self.tool_manager.invoke_tool(response)
                self.logger.log_output(tool_output)
                if tool_output['status'] == "Success":
                    self.conversation.add_message("user", tool_output['output'])
                    self.logger.log_status(tool_output['output'])
                    if "File written successfully" in tool_output['output']:
                        self.conversation.add_message("user", "If you have finished generating your test, use the Run tool to check it.")
                        self.logger.log_status("If you have finished generating your test, use the Run tool to check it.")
                else:
                    self.conversation.add_message("user", f"Tool invocation failed: {tool_output['output']}")
                    self.logger.log_status(f"Tool invocation failed: {tool_output['output']}")
            elif "<DONE>" in response:
                self.logger.log_status("Repair completed.")
                break
            else:
                continue_message = ("Your output doesn't contain a <TOOL>...</TOOL> invocation."
                                    " If you have generated, run and checked your test, respond <DONE>.")
                self.conversation.add_message("user", continue_message)
                self.logger.log_status(continue_message)

        if self.conversation.messages[-1]['role'] != "assistant":
            self.logger.log_failure("Repair failed to produce a valid response.")
            return
        self.logger.log_success("Repair completed.")
        return
